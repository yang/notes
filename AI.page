TODO

- mege in notes from CS182, 6.867
- Why do kernel smoothers divide the norm by λ before applying the weight fn? Isn't it dealt with in the summation in the estimator fn?
- kalman filters, smoothing
- <http://measuringmeasures.com/blog/2010/3/12/learning-about-machine-learning-2nd-ed.html>
- separating hyperplane
- sigmoid kernel function
- k-means++
- <http://hunch.net/?p=224>
- <http://googleresearch.blogspot.com/2010/04/lessons-learned-developing-practical.html>
- top 10: <http://gettinggeneticsdone.blogspot.com/2010/04/top-10-algorithms-in-data-mining.html>
- k-means/canopy clustering
- model fitting: linear models, regularization/overfitting, $k$-fold
  cross-validation
- model selection criteria: akaike information criterion (AIC), minimum
  description length (MDL), bayesian information criterion (BIC), ridge
  regression
- common distributions: normal, beta binomial, multinomial
- $t$-test: matched pairs and unmatched
- L1/lasso, L2/ridge/Tihkonov regression
- mixture model for collab filtering
- low-rank matrix factorization

wisdom

- data dredging aka data fishing aka data snooping: inappropriate use of data
  mining to uncover misleading relationships in data
  - can occur when no hyp formed in advance or reduced data reduces prob of
    refuting some hyp
  - when doing many significance tests, expect that some results are
    significant by chance alone
- no free lunch theorem: many hyps can fit; always assuming *something*
  - bias-free learning is futile
  - for search/optimization: all algos perform the same on avg (over all
    possible cost functions); improving perf requires prior info to match
    procedures to problems
  - for compression: "No compression algorithm can compress data on average."
  - <http://www.aihorizon.com/essays/generalai/no_free_lunch_machine_learning.htm>
- curse of dimensionality: volume grows exponentially with dimensions, so
  everything is far away

learning taxonomies

- great resource:
  <http://machine-learning.martinsewell.com/machine-learning.pdf>
- parametric v nonparametric
  - parametric learning: models have params that we're estimating, eg Bayesian
    learning; hyp complexity fixed
    - once params learned, they're all you need to predict
  - nonparametric learning: hyp complexity can grow with data
    - instance-based learning aka memory-based learning: hyps directly
      constructed from training instances, eg nearest-neighbor, kernels
    - eg LWR: must keep training set around; "re-training" all the time

data set balance

- class imbalance is common problem in many problems
- most common techniques
  - random undersampling (RUS) of majority class
    - shouldn't affect ROC curve
    - don't undersample test set
  - random oversampling (ROS) of minority class: duplicate samples
- synthetic minority over-sampling technique (SMOTE)
  - "informed over-sampling": for each minority sample $s$:
    - find $k$ nearest neighbors
    - randomly select $j$ of them ($j$ depends on oversampling desired)
    - randomly generate synthetic samples along the lines joining $s$ and the
      $j$ neighbors
  - shortcomings
    - may overgeneralize bc it disregards majority class; esp problematic for
      highly skewed class distributions bc minority class is very sparse wrt
      majority class
    - inflexibility: $j$ is fixed in advance
  - adaptive synthetic minority oversampling (ASMO): overcomes shortcomings
  - <http://www.iro.umontreal.ca/~lisa/workshop2004/slides/japkowicz_slides.ppt>
  - may combine SMOTE and undersampling
- <http://metaoptimize.com/qa/questions/232/does-subsampling-bias-the-auc>
- <http://www.machinelearning.org/proceedings/icml2007/papers/62.pdf>

ensemble methods

- decision stumps aka weak learners: predict based on just one feature
- bootstrap aggregating aka bagging
  - given a learner (model), trains $t$ classifiers (instances of the learner),
    each on some bootstrap sample from the training data
  - bootstrap sample: sample w replacement (replace each example)
  - given a test example, classifiers take a vote (regressions take avg)
    - averages predictions, so not useful for improving linear models
  - error estimate: accuracy of the classifiers on their excluded sets
- boosting (adaboost): usu applied to decision trees
  - given a learner, trains $t$ classifiers (instances of the learner) on
    samples of the training data
  - when a classifier is trained, weights for misclassified instances are
    increased, so that next classifier must try harder to get those right
  - classifiers also assigned a weight based on accuracy on training set
  - given a test example, classifiers take a weighted vote
- rely on randomization/perturbation to build diverse models, so only unstable
  learners can be used; not useful for stable learners like SVM, NB
  - stable learners: produce similar models upon small perturbations to
    training set

supervised learning

- classification: discrete/categorical
- regression: continuous
- ordinal regression: ranking (in btwn classification & regression; used in IR)

- old statistically inspired linear methods
- neural networks
  - nodes aka units are connected by links propagating activations
  - each node is activation function of weighted inputs
  - activation functions: threshold or sigmoid/logistic (differentiable)
  - bias weight $W_{0,i}$ connected to fixed input $a_0 = -1$, sets actual
    threshold
  - acyclic/feed-forward networks or cyclic/recurrent networks (memory)
  - hidden units/layers: internal units/layers
  - FF networks usu arranged in layers
  - perceptron: single-layer FF using threshold function; linear
  - multilayer backpropagates errors to hidden layers by assuming each hidden
    node responsible for some fraction of the error in each of the output nodes
    it connects to
- gradient descent for logistic regression *is* perceptron with sigmoid/logit
- decision trees
  - pruning: remove irrelevant features; eg $\chi^2$ pruning (math)
  - finding split points on continuous features is usu most expensive part
    (math)
  - ID3: standard simple learning algo
    - choose attr w max info gain (or, equivalently, min entropy)
  - C4.5: standard modern decision tree learner
    - supports continuous attrs, missing values, diffing costs, pruning
      irrelevant attrs
- support vector machines (SVM) aka kernel machine
  - maximum margin classifiers, potentially with slack
  - linear: similar to logistic regression
- Gaussian processes

- $k$-nearest-neighbors (KNN): classify objects based on closest $k$ neighbors

- case-based reasoning
  - knowledge-based ai
  - statistical ml
- anytime learning: online continuous learning

- margin infused relaxation algo (MIRA): TODO
  - <http://aria42.com/blog/?p=216>

graph search

- BFS, DFS, IDFS
- best-first search: explore most promising node according to heuristic fn
  - usu.: minimize distance to goal
- A* search: best-first where heuristic fn is distance to goal + distance from
  start
  - unlike best-first min-dist, may jump off a certain path you've been heading
    down to keep from going too far off
- beam search: best-first but at ecah step keep limited # of best candidates
- dijkstra: exact min cost path
  - orig algo $O(|V|^2)$; using Fib heap $O(|E| + |V| \log |V|)$
  - for each node, maintain a "min known distance from root"
  - starting from root, relax neighbors' distances & put into/pop from PQ
  - once all neighbors of a node explored, dist won't relax again
  - neg edge weights cause inf loops
  - pseudocode:

      for x in nodes:
        x.dist = inf
        x.prev = null
      root.dist = 0
      q = PQ({root})
      while x = q.pop():
        for y in x.neighbors:
          m = min(y.dist, x.dist + edge(x,y))
          if m < y.dist:
            y.dist = m
            y.prev = x // record path
            insert or relax y in q

- bellman-ford: single-source shortest-paths all-pairs
  - same as dijkstra but instead of greedily relaxing min-dist unvisited node,
    relax *all* edges, and can report neg edges (no 'shortest path')

    for v in vs: v.dist, v.pred = inf, null
    for i in range(len(vs)):
      for u,v in es:
        if u.dist + edge(u,v) < v.dist:
          v.dist = u.dist + edge(u,v)
          v.pred = u
    for u,v in es:
      if u.dist + edge(u,v) < v.dist:
        raise Exception('graph contains neg weight edges')

constraint satisfaction

- unification
  - backtracking

local search/optimization algos

- hill climbing: each step adjusts a single element in the vector; various ways
  to choose successor
- simulated annealing: adjusts all elements in the vector according to gradient
  of the hill
- local beam search: hill-climbing in lock-step; share info so that each
  exploring thread jumps to one of the best successors found by any of the
  other threads
- stochastic beam search: local + 
- minimax
- nearest neighbors
  - knn: see above
  - cuckoo hashing
- genetic algos

nearest-neighbor

- density estimation: defines joint dists over input space
- unlike Bayes net, no hidden vars/unsupervised clustering
- distance functions
  - continuous features: don't just use Euclidian; normalize by standard
    deviation ($Z$-distance)
    - special case of Mahalanobis distance, which uses covariance as well
  - discrete features: Hamming distance
- can predict some attr $y$ with $P(y|x) = P(y,x) / P(x)$
- supervised learning: predict $y=h(x)$ from $k$ nearest neighbors of $x$
  - for discrete: majority vote
  - for continuous: average, or do local linear regression using the $k$ points
- high-dimensional problems
  - slow kNN search
  - curse of dimensionality on distance functions

clustering

- difference from partitioning: clustering is on points that have coordinates
  or distances, rather than shapeless graphs
- hierarchical
  - agglomerative: bottom-up merging
  - divisive: top-down splitting
- partitional: determine all clusters at once; can be used in divisive
  - $k$-means: assign each point to cluster with closest centroid; initialize with random centroids
    - can use results to determine _clustroids_ (points closest to centroids)
  - locality sensitive hashing (LSH): hash similar values to similar values
    - a form of dimensionality reduction
    - also used in nearest neighbor search
    - feature space vectors are sets
    - jaccard distance
    - random projection:
      - each value is a vector in hyperspace
      - divide hyperspace with a hyperplane to classify the values in 1 bit
      - repeat for more bits
    - min-hash/sim-hash is an instance of this
    - <http://knol.google.com/k/simple-simhashing>
- density-based: discover arbitrary shapes
- 2-way-/co-/bi-clustering: objects are clustered but also their features; if
  data in matrix, then rows and columns clustered simultaneously
- distance measures: euclidian/2-norm, manhattan/1-norm, maximum norm aka
  infinity norm, hamming, inner product/cosine similarity
- many clustering algos require number of clusters as input; 

machine learning

- linear classif, perceptron, SVM
- non-linear classif, kernels
- n-way classif, rating, ranking
- anomaly detection
- (logistic) regression
- collab filt
- feat sel
- ensembles, boosting
- active learning
- model sel, complexity
- VC-dim, generalization guarantees
- mixture models, EM
- topic models, markov models, HMMs
- bayesian nets (and learning them)
- markov random fields, factor graphs, inference
- inference, belief propagation
- inference, junction trees
- conditional models, structured prediction
  - structured prediction: similar to classification but predicting an output
    with many possible values but still tractable to learn, e.g. structures;
    eg POS tags for a string of words
- learning conditional models

Developing a self-driving car for the 2007 DARPA Urban Challenge (Seth Teller's
Angstrom talk, 5/1/08)

- 40 compute cores
- many other teams performed human-assisted map annotation
- Seth's team focused on going almost entirely from sensing
- many other teams used much fewer cores
- uses of sensing
  - using vision for finding paint
  - using active sensing for depth
- stereo algos: reconstruct 3D from cameras
  - recent stereo algos have a global component that is harder to parallelize
- optical flow: estimating motion across time
- groups
  - planning & control: how to stay on the white dotted lines; one technical
    focus
  - perception: another technical focus
  - working on the car
  - software infrastructure: codebase, OS, etc.
- power-bound
  - also CPU-bound in the sense that they could process rawer images, at higher
    rates, etc.
  - also IO-bound; had 2 GigE and a CAN, all mostly utilized
- Anant: near future will yield cameras that have encoders
  - Seth: these are not as useful for vision research
  - algos need to know gradients, for instance; what does MPEG do to gradients?
- looked at GPUs
  - programming models still painful
  - form factor limits (1U blades)
- didn't want DSPs either because they didn't know what algos they ultimately
  wanted; everything was exploratory
- Anant: suggest fifth team, on computation resources

MACHINE LEARNING

- variables
  - visibility
    - _hidden variables_, _latent variables_, _model parameters_, _hypothetical variables_
    - _observable variables_, _manifest variables_
  - causality
    - _independent variable_, _predictor variable_, _regressor_, _controlled variable_, _manipulated variable_, _explanatory variable_
    - _dependent variable_, _response variable_, _regressand_, _measured variable_, _responding variable_, _explained variable_, _outcome variable_
  - TODO any diff btwn visibility/causality?

- bayes vs frequentist
  - TODO diff btwn | and ;?

- models

- data mining

- analyses
  - mathematical
    - real analysis
    - complex analysis
  - statistical
    - analysis of variance (ANOVA)
    - time-series analysis
  - latent variable model
    - factor analysis
    - latent trait analysis
    - latent profile analysis
    - latent class analysis
  - PCA
  - exploratory data analysis

feature/dimensionality reduction

- feature selection
- feature extraction
  - principal component analysis (PCA)
- autoencoder aka sparse coding

principal component analysis (PCA)

- use covariance method or SVD (preferred)
- covariance method:
  - `row_data_adjusted = row_data - mean(row_data)` (normalized to have unit
    variance?)
  - find eig of compute covariance (correlation) matrix
  - choose eigenvectors that have biggest eigenvalues; these are the
    (principal) components
  - _feature vector_ is matrix of col evecs $[e_1, e_2, \dots, e_n]
  - `transformed_data = trans(feature_vector) * row_data_adjusted`
- components optimally account for variance in features
  - first component extracted accounts for max amt of total variance
    - i.e., choose new axis covering greatest "spread" in data points
    - i.e., choose new axis minimizing L2 errors
    - typically means it will be correlated w several/many vars
  - second component extracted accounts for max amt of variance not
    accounted for by first component
- results depend on scaling of variables
- eval directly corresponds to fraction of total variance
  - eg if total variance of 10, eval of 6.1 means 61% of total variance
- can extract _orthogonal_ components or _oblique_ components (which may
  produce cleaner results but also may be more complicated to interpret)
- *not* factor analysis; makes no assumptions about underlying causal model or
  latent vars; simply reduce to components that account for most of the variance
  - but may sometimes produce similar results
- optimal orthogonal xform but more computationally expensive than eg DCT
- refs
  - <http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf>
  - <http://support.sas.com/publishing/pubcat/chaps/55129.pdf>

factor analysis

- for when you believe latent factors cause observations

linear discriminant analysis

- alt to PCA that optimizes for class separability
- TODO
- despite name, it's generative; name makes sense vs PCA (which ignores labels)

singular value decomposition (SVD)

- SVD of $M$ is a factorization of the form $M = U \Sigma V*$ where:
  - $U$ is $m \times m$ real/complex unitary matrix
  - $\Sigma$ is $m \times n$ nonnegative real diag matrix
  - $V*$ is $n \times n$ real/complex unitary matrix
- when all real, then $U,V$ are rotations and $\Sigma$ is scaling
- eval decomp is similar but only for square matrices
- use SVD to perform PCA
  - covariance matrix is $\frac{1}{n} X X^T = \frac{1}{n} U \Sigma^2 U^T$

- resources
  - http://videolectures.net/icmi05_bengio_tsmla/
  - google video: machine learning

trending algorithms

- simple baseline trend algorithm: daily trend = most recent day's change *
  log(monthly total)
  - <http://stackoverflow.com/questions/1635703/understanding-algorithms-for-measuring-trends>

decision tree learning algorithms TODO

- NP complete

generalization guarantees

- bias TODO
- variance TODO
- Vapnik-Chervonenkis (VC) dimension
  - model _shatters_ some data if for all labelings there's some perfect params
    (no errs)
  - VC dim of model is size of largest set that model can shatter
  - eg linear classifiers: VC-dim of 3
  - can predict probabilistic upper bound on test err of classification model
    TODO

comparing classifiers

- not sure how trustworthy the following is, but mostly accurate
- high-bias/low-var (eg NB) better for small training sets
- low-bias/high-var (eg kNN, LR) better for bigger training sets
- NB: simple; fast; converges faster than discriminative model like LR; good
  for small datasets
- LR: many ways to regularize, so less worry about correlated features; nice
  prob interp useful for adjusting thresholds or getting confidence scores;
  easily update model w online grad desc, unlike SVMs/DTs
- DT: easy to explain; non-param; RFs avoid overfitting; RFs beat SVMs
- SVM: accurate; slow; too many kernels/params
- <http://www.quora.com/What-are-the-advantages-of-different-classification-algorithms>

generalization error

- cross validation, bootstrapping: methods to estimate generalization error;
  based on "resampling"
  - often used to choose among different models; e.g. different neural network
    architectures; choose one with lowest gen error
- cross validation
  - *doesn't* estimate error on future data
  - rather, a way to pick best learning algo (and best learning params that
    aren't already being numerically optimized)
  - $k$-fold cross validation: divide data into $k$ subsets of roughly same
    size (folds)
  - train $k$ times, each time leaving out one of the subsets, then testing on
    one subset
  - LOOCV: when $k$ is the sample size ($k=n$ where $n$ is data size)
  - leave $v$ out cross validation: more elaborate and expensive that leaves out
    all possible subsets
  - diff from "split-sample"/"hold-out" method commonly used for early stopping
    in neural networks
    - only 1 fixed subset (the validation set) used to estimate generalization
      error, not $k$ subsets; no "crossing"
  - cross-validation to split-sample is superior for small datasets
  - LOOCV good for continuous error functions/model-selection methods, bad for
    discontinuous ones
  - repeated random sub-sampling validation (this may be referred to as
    bootstrapping)
    - split multiple times randomly; avg over splits
    - split multiple times randomly; avg over splits
  - stratification
    - repeated random sub-sampling validation
      - maintain same mean response value is equal in training and test sets
        TODO understand that sentence
      - useful if responses are dichotomous, where data has unbalanced repr of
        the two response values
    - stratified $k$-fold CV
      - choose folds so that mean response value is approx equal in all folds
        TODO understand that sentence
      - for dichotomous classification, each fold has ~same +/- labels
- jackknifing: LOOCV but for estimating bias not gen error
  - compute some statistic of interest in each subset
  - compare avg of these to avg of entire sample to estimate the latter's bias
- bootstrapping
  - repeatedly analyze subsamples of the data rather than subsets
  - ea subsample is a random sample with replacement from full sample
    - subsamples may be of same cardinality as full sample
  - allows us to evaluate estimators
    - example from classical statistics: what's the variance of the mean? what
      are our confidence intervals?
    - example from machine learning: for a decision tree algo, evaluate mean &
      variance of (# correct)/(# elements)
  - TODO clarify relationship w CV
    - seems CV is better for ML tasks, since test set != train set
    - bootstrapping works better than CV in many cases?
      <http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html>
- ref: <http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html>

ML vs statistics

- <http://www-stat.stanford.edu/~jhf/ftp/dm-stat.pdf>
  - statistics too focused on math technique rather than ideas
  - empirical validation and mathematics are complementary
  - stats: prob theory, real analysis, measure theory, asymptotics, decision
    theory, markov chains, martingales, ergotic theory
    - tools: hyp testing, experimental design, response surface modeling,
      ANOVA/MANOVA, linear regression, discriminant analysis, logistic
      regression, GLM, canonical correlation, principal components, factor
      analysis
  - DM
    - tools: decision trees, rule induction, nearest neighbors, clustering,
      association rules, feature selection, neural nets, graphical models,
      genetic algos, self-organizing maps, neuro-fuzzy systems
- <http://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/>

receiver operating characteristic (ROC)

- based on confusion matrix aka contingency table
- true positive rate aka TPR aka sensitivity: TP/P or TP/(TP+FP)
- false positive rate aka FPR aka 1 - specificity: FP/N or FP/(TN+FN)
- ROC curve: plot of FPR on x and TPR on y while varying threshold
  - input: true labels and scores from classifier
  - top left is perfect; random tends to be around FPR=TPR diagonal
  - python to generate a ROC curve

      # say sorted(zip(scores, labels), reverse=True) returns:
      # [(.9, 1), (.8, 1), (.6, 0), (.4, 1), (.2, 0), (.1, 0)]

      pos, neg = labels.count(1), labels.count(0)

      # simple implementation: doesn't sort data first

      thresholds = sorted(set(scores), reverse=True)
      for i, t in enumerate(thresholds):
        # num examples with predict=1 and label=1
        tp = sum(label for score, label in zip(scores, labels)
                 if score >= t and label == 1)
        # num examples with predict=1 and label=0
        fp = sum(label for score, label in zip(scores, labels)
                 if score >= t and label == 0)
        yield tp/pos, fp/neg

      # more efficient implementation sorts data first and accumulates data

      tp, fp = 0
      # this descends the threshold to each distinct value of score
      for score, label in sorted(zip(scores, labels), reverse=True):
        if label: tp += 1
        else: fp += 1
        yield tp/pos, fp/neg

- area under curve (AUC) metric
  - random classifier: AUC = .5
  - perfect classifier: AUC = 1
  - different apps have different prefs
    - spam filtering: want low FP
    - medical screening tests: want high TP
- AUC has implicit cost functions; diff cost distributions for diff classifiers
  - don't use AUC to compare diff classifiers
  - <http://www.springerlink.com/content/y35743hp7010g354/>
- <http://metaoptimize.com/qa/questions/4370/how-to-generate-a-roc-curve-for-binary-classification>
- <http://metaoptimize.com/qa/questions/232/does-subsampling-bias-the-auc>

loss functions

- 0-1 loss
- pairwise cross entropy
- lambda loss
- pairwise + RMSE

adaboost

- idea: combine weak classifiers

  initially, all examples equally weighted
  for some number of rounds,
    find a weak classifier that minimizes weighted error
    if weighted error >=50% accuracy, break
    down-weight correct examples, up-weight incorrect examples
  combine subordinate models into final model,
    each weighted inversely by training error

learning bayes net structures

- still active research
- search algo: modifying an initial structure (nodes, arcs)
  - some assume partial topological order over vars is given
- criteria
  - accuracy after fitting params
  - if implicit conditional independence assertions are satisfied
  - MLE results in fully connected network; must penalize with bayesian
    (MAP/MDL) approach over network hyps (prefer simpler networks)
    - usu too many structures to sum over, so sample w MCMC

graphical models

- bayesian network aka belief network aka directed graphical model
  - local markov prop: any 2 nodes conditionally indep given parents' values
  - edges usu repr causality but not required (either dir)
  - plates: for repeating sub vars
  - eg naive bayes, neural nets
  - _markov blanket_: neighbors; parents, children, children's parents
    - every node is conditionally indep of all other nodes in network given
      blanket
  - dynamic bayesian network: for sequences of vars, eg time series
    - eg HMMs, kalman filter
- markov network aka markov random fields aka undirected graphical model
  - reprs some things bayes nets can't (cyclic deps) & vice versa (induced deps)
  - eg log-linear models, gaussian markov random field TODO
  - must satisfy pairwise, local, global markov props TODO
  - conditional random fields TODO
- factor graph
  - bipartite hypergraph representing factorization of a function TODO
- <http://cacm.acm.org/magazines/2010/12/102122-bayesian-networks/fulltext>

linear classifiers

- naive bayes
  - generative
  - it's actually linear: <http://nlp.stanford.edu/IR-book/html/htmledition/linear-versus-nonlinear-classifiers-1.html>
    - choose $c_0$ iff $\log \frac{P(c_0|d)}{P(c_1|d)} = \log
      \frac{P(c_0)}{P(c_1)} + \sum_{k=K} \log \frac{P(t_k|c_0)}{P(t_k|c_1)} >
      0$ where $K$ is the set of terms in the document
- logistic regression
  - discriminative
  - for $K$ classes, $P(y|x) = \frac{ \exp(w_y \cdot x) }{
    \sum_{y'\in\{1,\dots,K\}} \exp(w_{y'} \cdot x) }$
- <http://www.quora.com/What-is-the-difference-between-logistic-regression-and-Naive-Bayes>

generative vs discriminative

- generative: LDA, NB
- discriminative: LR, perceptron, SVM

GLM

- families
  - Bernoulli and binomial: logistic regression
  - Gaussian: OLS regression
  - multinomial: softmax regression
  - Poisson: for modelling count-data
  - gamma and exponential: for modelling continuous, non-neg RVs, eg time
    intervals
  - beta and Dirichlet: for dists over probs
  - many more...

maximum entropy

- _max entropy classifier_ aka _multinomial logit_
  - generalizes logistic regression
  - assumes independence of irrelevant alternatives (IIA): eg choices are blue
    bus, yellow bus, car, but most ppl choose btwn (any) bus and car
  - unlike alts like naive bayes, doesn't require feature independence, but
    learning is much slower
  - $P(y_i=j) = \frac{\exp(X_i \beta_j)}{1 + \sum_{j=1}^J \exp(X_i
    \beta_j)}$ and $P(y_i=0) = \frac{1}{1 + \sum_{j=1}^J \exp(X_i \beta_j)}$
  - $\beta_j$ typically estimated using maximum a posteriori (MAP), which is an
    extension of max likelihood using regularization of weights

gibbs sampling

- an MCMC algo (special case of Metropolis-Hastings algo)
- purpose: given a multivariate dist, simpler to sample from a conditional dist
  than the joint (hard to marginalize by integrating over joint dist)
- denote $i$-th sample w $X^(i) = \{ x_1^(i), \dots, x_n^(i) \}$ from a joint
  dist $p(x_1, \dots, x_n)$
- start w some $X^(0)$
- for each sample $i=\{1, \dots, k\}$, sample each var $x_j^(i)$ from cond dist
  $p(x_j^(i) | x_1^(i), \dots, x_{j-1}^(i), x_{j+1}^(i-1), \dots, x_n^(i-1))$
  - cond on all other vars, making use of most recent values and updating ASAP

feature selection

- subset selection: evaluate a subset of features (or: which feature to discard
  first? a search problem)
  - wrappers: wrap a learning model, may overfit
    - aka least angle regression (LARS) bc at each step you choose the
      dimension that, when added, minimizes your angle to the residual ($y$)
    - forward: starting w empty set, try adding features one/some at a time,
      greedily choosing the feature that yields the highest cross-validation
      accuracy
    - stepwise: like forward, but after each add, also optionally remove a
      feature, if removal yields a higher accuracy
    - backward: a faster and sloppier version of naive backward (which is
      opposite of forward)

        start with all features in model
        candidates (for removal) = all features
        for each iteration,
          remove from candidates any feature whose exclusion yields no acc drop
          remove (from model and candidates) feature in candidates with biggest acc drop

    - recursive feature elimination: another name for backward
  - filters: like wrappers, but instead of model, use simpler filter
- <http://www.quora.com/Regression-statistics/What-is-Least-Angle-Regression-and-when-should-it-be-used>
- TODO stepwise regression
- TODO optimality criteria

recommender systems

- some flavors: depends on how much data you have
  - collaborative filtering: works best in isolation but requires enough data
    to combat sparsity
  - content-based
  - social/trust: based on network's items/activity
  - usually mixing is good
- "the science and the magic of user and expert feedback for improving
  recommendations" (telefonica)
  - feedback is noisy; users inconsistent, esp. for mild ratings; re-ratings
    very effective in increasing prediction acc (choose milder re-rating)
  - expert-based recs have many benefits
  - [video](http://sna-projects.com/blog/2011/04/improving-recommendations/)
  - [slides](http://www.slideshare.net/xamat/the-science-and-the-magic-of-user-feedback-for-recommender-systems)
- if $M$ is ratings matrix with users in rows and movies in cols, then:
  - for $M M^T$: one evec may be for "horror" movies, and $i$th component
    is user $i$'s pref for horror movies; evals are feature importance
  - for $M^T M$: one evec may be for "horror" movies, and $j$th component is
    how horrific movie $j$ is

collaborative filtering

- various similarity measures
  - jaccard for binary features
  - cosine similarity
  - pearson correlation: useful if want insensitivity to users' differing
    scales
- item-item similarity (first pub by glinden et al)
  - predict a user's rating of an item as the weighted sum of other items' ratings by the user, where weights are item similarities:

      score[usr,itm] = avg(sim(itm,itm2) * score[usr,itm2] for itm2 in itms)

- user-user similarity
  - predict a user's rating of an item as the weighted avg of other users'
    ratings for the item, where weights are user similarities:

      score[usr,itm] = avg(sim(usr,usr2) * score[usr2,itm] for usr2 in usrs)

  - <http://software-carpentry.org/4_0/matrix/recommend/>
- evaluation metrics: RMSE is typical
- TODO slope one

random forests

- bagged ensemble of decision trees (orange uses 100 by default)
- each decision tree uses subset of all features (sqrt of all features)
- each decision tree may be grown fully without pruning
- each decision tree trained on a bootstrapped sample of the data (as per
  bagging)
- their reported error (confidence?) does take into account their performance
  on the training set, somehow
- may be obvious but: despite feature sampling, feature selection still helps,
  since the trees are not weighted at all by their performance (if they were,
  then they'd implicitly be weighing their own subset of features)
- efficient, parallelizable
- RFs is very competitive, but also has danger of overfitting
- can rank features with this rough technique:
  - each tree is trained over a bootstrapped sample, so check tree accuracy on
    the complement ("test set")
  - for each feature, randomly permute the values over the records, and
    re-measure accuracy
  - rank/score features by how much its randomization affected tree accuracy
  - accumulate scores over all trees

multiclass classification

- some classifiers handle it naturally
  - dec trees, bayes methods, log reg, ...
- generic meta learners
  - 1 vs rest aka 1 vs all aka OVA: build classifier for each class where pos
    iff labeled w that class; in test, choose class w highest classifier score
  - 1 vs 1 aka pairwise aka all vs all aka AVA: build classifier for each pair
    of classes ($\choose{n,2}$ classifiers); in test, choose class w most wins
  - AVA builds more classifiers but pairwise classification much faster (?) and
    more balanced (easier to find best regularization), hence often faster than
    OVA
    <http://www.cs.berkeley.edu/~pliang/cs294-spring08/lectures/multiclass/slides.pdf>
  - error-correcting output codes (ECOC)
    - each class given unique binary str of length $n$
    - train $n$ classifiers, one per bit
    - in test, run $n$ classifiers on $x$ to get $n$-bit str
      - coding must be carefully chosen to maximize hamming dist among
        codewords and make sure each bit is uncorrelated w other bits
      - choose coding using exhaustive codes (all $2^{k-1} - 1$ bits), column
        selection from exhaustive codes, randomized hill climbing, BCH codes,
        more
    - choose class w closest str (eg by hamming dist)
  - tutorial
    <http://courses.washington.edu/ling572/winter2011/teaching_slides/2_15_multiclass_to_binary.pdf>
  - generalization:
    <http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=B6C31DC721E6484E27B9380C764CBF98?doi=10.1.1.73.2154&rep=rep1&type=pdf>
    - ECOC is a less-general intermediate
    - orthogonal probs: how to classify example or assign it a prob dist given
      classifiers' results
  - In Defense of One-Vs-All Classification
    <http://jmlr.csail.mit.edu/papers/volume5/rifkin04a/rifkin04a.pdf>

latent dirichlet allocation (LDA)

- learns from body of entities, inferring that a particular attr belongs to a
  topic
- TODO

systems
=======

gmail priority inbox

- <http://research.google.com/pubs/archive/36955.pdf>
- concise paper
- use simple linear logistic regression for scalable learning & prediction
- features: social (sender/recip metrics), msg content, thread, labels
- importance metric: $p=P(a \in A, t \in (T_{min}, T_{max}) | \mathbf{f}, s)$
  - action $a$ within time $t$ of delivery given whether seen ($s$)
  - $T_{min}$ under 24 hours, constrained by model update frequency and gives
    users time to react
  - $T_{max}$ in days; longer interaction periods are not considered
  - [prediction error strange]
- personalizing model uses transfer learning
  - $s = \sum_{i=1}^n f_i g_i + \sum_{i=1}^{n+k} f_i w_i, p=\frac{1}{1 +
    \exp^{-s}}$
  - $k$ user-specific features
- use PA-II online passive-aggressive regression variant
- misc
  - continuous -> binary features via ID3-style algo [?]

misc

- transfer learning: storing knowledge gained while solving one problem and
  applying it to a different but related problem
  - eg useful for personalization
  - simple form: final prediction = sum of global and local (user-specific)
    models

nlp
===

- segmentation: figuring out word boundaries
  - simplest: over all possible segmentations, return seg w max prod over all
    words of P(word)

modeling
========

- hyperparameters: model params that are not being estimated/optimized

svm
===

general

- hyperparameters
  - eg $C$ penalty param
  - eg kernel-specific params; eg poly kernel has more hyperparams than others
- slack variables introduce soft margin

kernels

- from <http://anyall.org/blog/2011/01/please-report-your-svms-kernel/>:

  > An un-kernelized, linear SVM is nearly the same as logistic
  > regression...But a quadratic kernelized SVM is much more like boosted
  > depth-2 decision trees.  It can do automatic combinations of pairs of
  > features — a potentially very different thing, since you can start throwing
  > in features that don’t do anything on their own but might have useful
  > interactions with others.  (And of course, more complicated kernels do
  > progressively more complicated and non-linear things.)
  >
  > I have heard people say they download an SVM package, try a bunch of
  > different kernels, and find the linear kernel is the best. In such cases
  > they could have just used a logistic regression.  (Which is way faster and
  > simpler to train!  You can implement SGD for it in a few lines of code!)
  >
  > A linear SVM sometimes has a tiny bit better accuracy than logistic
  > regression, because hinge loss is a tiny bit more like error rate than is
  > log-loss.  But I really doubt this would matter in any real-world
  > application, where much bigger issues are happening (like data cleanliness,
  > feature engineering, etc.)
  >
  > If a linear classifier is doing better than non-linear ones, that’s saying
  > something pretty important about your problem.  Saying that you’re using an
  > SVM is missing the point.  An SVM is interesting only when it’s kernelized.
  > Otherwise it’s just a needlessly complicated variant of logistic
  > regression.

regression

- [question: what does the objective function
  mean?](metaoptimize.com/qa/questions/4151/understanding-svm-regression-objective-function-and-flatness)
- <http://kernelsvm.tripod.com/>
- [Tutorial on SVM
  Regression](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.2073&rep=rep1&type=pdf)

pragmatics/usage

- scale params to avoid feature imbalance and numeric difficulties
- kernels
  - RBF: good first choice
    - in (0,1], so fewer numerical difficulties
    - just one hyperparam ($\gamma$)
    - linear is special case of RBF; behaves like sigmoid for certain params
  - linear: good when many features
- simplest approach to finding hyperparams: grid search

multiclass

- <http://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html>
- generic approaches (OVA, AVA, etc) "not very elegant"
- instead, build 2-class SVM over new feature vector that *includes the class
  as a feature* (or in general some function of it, $\Phi(x,y')$)
  - in test, choose class w largest margin $y = \arg \max_{y'} w^T \Phi(x,y')$
  - in train, margin is gap btwn this value for correct class and for the
    nearest other class, so new quadprog formulation required (see URL)

ranking

- <http://nlp.stanford.edu/IR-book/html/htmledition/result-ranking-by-machine-learning-1.html#sec:ranking-svm>

rule learning/mining/induction

- apriori: best-known algo for association rule mining
  - for items that are grouped in txns
  - build up candidate itemsets one item at a time using BFS, but on each
    iteration prune out candidates, keeping only those with minimum _support_
    (frequencies) and (optionally) minimum _confidence_ (# in subset including
    new item / # in current set)
  - can also only expand search to candidates that have a minimum 
  - historically significant but has inefficiencies & trade-offs
- FP-growth: frequent pattern growth
  - param: min support
  - build FP-tree data structure
    - sort each txn (itemset) by item frequency
    - build prefix tree from txns
    - chain identical nodes so we can find all tree paths containing that item
  - to find frequent itemsets, for each distinct item $x$, find if it has
    enough support (sum of counts across all its nodes >= min support)
    - if so, yield that (+ suffix if currently recursing) as a frequent itemset
    - for each item $x$, build _conditional FP-tree for $x$_
      - FP-tree of itemsets ending in $x$, excluding $x$
      - build new FP-tree over paths collected by walking up parent ptrs
      - update node counts: sum of children's supports (anchor w $x$ node
        supports)
  - <http://www.engr.uvic.ca/~seng474/association3.ppt>
  - <http://www.engr.uvic.ca/~seng474/fptree_complete_example.ppt>
  - <https://github.com/enaeseth/python-fp-growth/blob/master/fp_growth.py>
  - orig paper <http://www.inf.unibz.it/dis/teaching/DWDM/project2010/FP-growth.pdf>
- AQ: outputs sequence of "if COND then CLASS" rules
  - beam search (best-first search with pruning)
  - once find rule, remove covered data, repeat
  - TODO diff from CN2?
- CN2: rule induction algo
  - based on ideas from the ID3 decision tree algo
  - significance test filters out rules that cover too few examples
  - possible heuristic criteria to rank rules in beam (best-first) search
    - entropy: used in ID3 and orig CN2
      - tends to select very specific rules covering few examples; significance
        test is only a threshold; tihs is called _downward bias_
    - accuracy: behaves similarly to entropy; $P(class|cond) = n_c/n$, where:
      - $n_c$ is # examples in predicted class $c$ covered by rule
      - $n$ is total # examples covered by rule
    - laplace accuracy: $\frac{n_c + 1}{n + k}$, where $k$ is # classes
      - not sure about rationale here
    - weighted relative accuracy: $P(cond) (P(class|cond) - P(class))$
      - instead of focusing on accuracy, focus on finding big groups
      - $P(cond)$ is _generality_; find big groups
      - $P(class|cond) - P(class)$ is _relative accuracy_
      - $= P(cond, class) - P(class) P(cond)$ (diff btwn actual joint and
        expected joint if indep)
      - laplace accuracy can be used for $P(class|cond)$
  - ordered v unordered: whether rule is defined conditioned on rules above it
    having failed
    - unordered algo change: iterate the search for each class in turn,
      removing only covered examples *of that class* when rule is found
  - CN2-SD (subgroup discovery): instead of removing, downweight
    - bc in CN2 only first few rules may be of interest
    - can decrease multiplicatively or additively
  - TODO how to know when to stop?

regularization

- overfitting tends to occur when large weights found in $\theta$
- add regularization term to pressure params $\theta$ to be 0
- L2: $\lambda \left\lnorm \theta^2 \right\rnorm$
  - $\lambda$ is the regularization param
  - special case of Tikhonov regularization
  - can be computed directly in $O(n^3)$
- L1: $\lambda \left\lnorm \theta \right\rnorm$
- TODO

backoff

- m-estimate: backoff to handle sparsity
  - instead of n_c / n where n_c is # matching examples in class c and n is #
    matching examples, use (n_c + m p_c) / (n + m) where p_c is relative class
    frequency (over all)
  - mitchell, 1997 / lidstone's law

getting in shape for the sport of data science (jeremy howard, kaggle chief scientist)

- subject: how to compete in kaggle
- coding to prep data for models is most important
  - else, you're just applying models
  - regexes get you very far
- excel is very powerful for data understanding
  - cell color scales
  - pivot tables
- programming platform recommendations
  - python, numpy, scipy, matplotlib, ipython
  - c#, alglib (ML), math.net numerics (linalg), chartcontrol (charts)
  - c++, eigen (powerful linalg)
  - java, weka
- R packages
  - caret: auto find models, grid search hyperparams, etc.

      mdl <- train(data, "svm", center, pca)

  - ggplot2: transparent scatterplot points
  - ggobi: like tableau
    - brushing: select pits in one plot, highlight in others
    - not used as much in practice bc excel/ggplot2 so good
- viz must be flexible more than fancy
  - eg plotting hundreds of time series is surprisingly useful (won that comp)
- models
  - parametric models: GLM
  - 
  - non-parametric models: SVM, RF
- traffic prediction: excel color scales helped see the pattern/propagation of
  jams
- cran recommendations problem
  - P(user installing any pkg) * P(pkg getting installed) -> 84% AUC
  - use max of probs -> 95% AUC
  - use RF and a bunch more data -> 93% AUC
  - use RF on max -> 83% AUC
  - GLM on a bunch more data -> 97% AUC
  - GLM + mark deps as installed (type of programming that's cumbersome in R, a
    "shit programming language"; used C#) -> 98%
  - 6th place
- <http://blog.kaggle.com/2011/03/23/getting-in-shape-for-the-sport-of-data-sciencetalk-by-jeremy-howard/>

deep learning TODO

- sparse coding aka dictionary learning aka autoencoder
- learn low-level features; similar to PCA, but yields not necessarily
  orthogonal basis
- multilayer feed-fwd neural networks using back-prop (with small hidden layer)
- <https://venefrombucharest.wordpress.com/2011/04/15/an-overview-of-dictionary-learning-terminology/>
- <http://www.stanford.edu/class/cs294a/>
